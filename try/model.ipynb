{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.model import Laplace_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/Users/yentsokuo/git_repo/WKN_SSO/try/../src/model.py:41: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1009.)\n",
      "  return F.conv1d(waveforms, self.filters, stride=1, padding='same', dilation=1, bias=None, groups=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2560])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 2560)\n",
    "\n",
    "WKN = Laplace_fast(32,64)\n",
    "WKN_input = WKN(input)\n",
    "WKN_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2560])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = input.squeeze()\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2560])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 2560)\n",
    "\n",
    "c0 = nn.Conv1d(1, 32, kernel_size=32, padding='same')\n",
    "z = c0(input)\n",
    "\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1280])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool of size=3, stride=2\n",
    "m1 = nn.MaxPool1d(2, stride=2)\n",
    "a = m1(z)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1280])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = nn.Conv1d(32, 16, kernel_size=32, padding='same')\n",
    "b = c1(a)\n",
    "\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 640])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = nn.MaxPool1d(2, stride=2)\n",
    "c = m1(b)\n",
    "\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 640])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = nn.Conv1d(16, 32, kernel_size=32, padding='same')\n",
    "d = c2(c)\n",
    "\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 320])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = nn.MaxPool1d(2, stride=2)\n",
    "e = m3(d)\n",
    "\n",
    "e.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 16])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "e = e.view(320, 32)\n",
    "BiGRU = nn.GRU(input_size=32, hidden_size=8, num_layers=1, bidirectional=True)\n",
    "output, hidden = BiGRU(e)\n",
    "\n",
    "# 訪問最後一個時間步的隱藏狀態的形狀\n",
    "print(output.shape)\n",
    "# 或者訪問整個序列的隱藏狀態的形狀\n",
    "print(hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5120])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = nn.Flatten(0)\n",
    "o1 = f1(output)\n",
    "\n",
    "o1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = nn.Linear(5120, 64)\n",
    "o2 = l1(o1)\n",
    "\n",
    "o2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = nn.Linear(64, 1)\n",
    "o3 = l2(o2)\n",
    "\n",
    "o3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.8523e-01],\n",
      "        [ 4.5005e-01],\n",
      "        [ 1.5834e-01],\n",
      "        [-9.8807e-01],\n",
      "        [-2.0539e-01],\n",
      "        [ 1.0252e+00],\n",
      "        [-3.6740e-01],\n",
      "        [-1.2244e+00],\n",
      "        [-2.3265e-02],\n",
      "        [-1.5839e+00],\n",
      "        [ 6.4034e-01],\n",
      "        [-1.9665e+00],\n",
      "        [ 1.7263e+00],\n",
      "        [-3.7292e-01],\n",
      "        [ 1.6391e+00],\n",
      "        [-3.6448e-01],\n",
      "        [-5.6928e-02],\n",
      "        [ 3.9751e-01],\n",
      "        [-8.9022e-02],\n",
      "        [-1.2072e+00],\n",
      "        [-1.5513e+00],\n",
      "        [ 1.7884e+00],\n",
      "        [ 8.3717e-01],\n",
      "        [ 2.5921e-01],\n",
      "        [ 3.7565e-01],\n",
      "        [ 4.5042e-01],\n",
      "        [ 1.2479e-01],\n",
      "        [-3.2192e-01],\n",
      "        [-1.5225e+00],\n",
      "        [-5.8179e-01],\n",
      "        [ 4.1697e-01],\n",
      "        [-2.8003e-04]])\n"
     ]
    }
   ],
   "source": [
    "di = torch.randn(32,1)\n",
    "di_squeezed = di.squeeze()\n",
    "\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-7.8523e-01,  4.5005e-01,  1.5834e-01, -9.8807e-01, -2.0539e-01,\n",
      "         1.0252e+00, -3.6740e-01, -1.2244e+00, -2.3265e-02, -1.5839e+00,\n",
      "         6.4034e-01, -1.9665e+00,  1.7263e+00, -3.7292e-01,  1.6391e+00,\n",
      "        -3.6448e-01, -5.6928e-02,  3.9751e-01, -8.9022e-02, -1.2072e+00,\n",
      "        -1.5513e+00,  1.7884e+00,  8.3717e-01,  2.5921e-01,  3.7565e-01,\n",
      "         4.5042e-01,  1.2479e-01, -3.2192e-01, -1.5225e+00, -5.8179e-01,\n",
      "         4.1697e-01, -2.8003e-04])\n"
     ]
    }
   ],
   "source": [
    "print(di_squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 320, 32])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(17,32,320)\n",
    "x_permuted = x.permute(0, 2, 1)\n",
    "\n",
    "x_permuted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2560])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from src.utils import *\n",
    "from src.model import LA_WKN_BiGRU\n",
    "\n",
    "pth_path = '/Users/yentsokuo/git_repo/WKN_SSO/src/your_model.pth'\n",
    "# 加载已经训练好的模型\n",
    "model = LA_WKN_BiGRU()  # 请替换为你的模型类\n",
    "model.load_state_dict(torch.load(pth_path))\n",
    "model.eval()  # 切换模型为评估模式\n",
    "\n",
    "# 准备输入数据（震动数据），这里使用示例数据，你需要根据你的数据结构进行调整\n",
    "input_path = '/Users/yentsokuo/git_repo/WKN_SSO/viberation_dataset/Test_set/Bearing1_3/acc_00001.csv'  # 一个(1, 2560)尺寸的震动数据\n",
    "input_data = pd.read_csv(input_path, header=None, names=['hour', 'minute', 'second', 'microsecond', 'horiz accel', 'vert accel'])\n",
    "inputs = input_data['horiz accel'].values.astype(float)\n",
    "inputs = min_max_scale(inputs).reshape(1, -1)\n",
    "inputs_tensor = torch.from_numpy(inputs.astype(np.float32))\n",
    "# input_tensor = torch.Tensor(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2560])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2560])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yentsokuo/git_repo/WKN_SSO/try/model.ipynb 儲存格 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yentsokuo/git_repo/WKN_SSO/try/model.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#使用模型进行预测\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yentsokuo/git_repo/WKN_SSO/try/model.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yentsokuo/git_repo/WKN_SSO/try/model.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     predicted_remaining_life \u001b[39m=\u001b[39m model(inputs_tensor)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yentsokuo/git_repo/WKN_SSO/try/model.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# 处理预测结果，这取决于你的任务\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yentsokuo/git_repo/WKN_SSO/try/model.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 在这个例子中，predicted_remaining_life是一个float，表示预测的剩余寿命\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yentsokuo/git_repo/WKN_SSO/try/model.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPredicted Remaining Life: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_remaining_life\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git_repo/WKN_SSO/try/../src/model.py:67\u001b[0m, in \u001b[0;36mLA_WKN_BiGRU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWKN(x)    \n\u001b[1;32m     68\u001b[0m     \u001b[39m# print(x.shape)    \u001b[39;00m\n\u001b[1;32m     69\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git_repo/WKN_SSO/try/../src/model.py:40\u001b[0m, in \u001b[0;36mLaplace_fast.forward\u001b[0;34m(self, waveforms)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilters \u001b[39m=\u001b[39m (laplace_filter)\u001b[39m.\u001b[39mview(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels, \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size)\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(waveforms\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 40\u001b[0m waveforms \u001b[39m=\u001b[39m waveforms\u001b[39m.\u001b[39;49msqueeze(\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(waveforms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilters, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m, dilation\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 2)"
     ]
    }
   ],
   "source": [
    "#使用模型进行预测\n",
    "with torch.no_grad():\n",
    "    predicted_remaining_life = model(inputs_tensor)\n",
    "\n",
    "# 处理预测结果，这取决于你的任务\n",
    "# 在这个例子中，predicted_remaining_life是一个float，表示预测的剩余寿命\n",
    "print(f'Predicted Remaining Life: {predicted_remaining_life.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
